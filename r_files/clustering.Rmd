---
title: "clustering"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret) 
library(ggplot2)
library(dplyr)
library(readr)
library(Hmisc)
library(ggfortify)
library(rpart.plot)
knitr::opts_knit$set(root.dir = normalizePath("C:/Users/buehl/Documents/classes/senior-research"))
```

```{r}
cluster_frame <- subset(lin_frame_clean, select=-c(ridreth3,riagendr,diq010,mult,asian,black,white,hispanic))
#### Looking at best number of Clusters:
#### calculate within-cluster distances as a function of cluster size. This will help determine best number of clusters
wss = list(15)
for (k in 2:15){ 
  wss[k] = sum(kmeans(cluster_frame[,-1], k, nstart=10)$tot.withinss)
}

### plot results 
plot(2:15, wss[2:15], type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
```


```{r}
#### creating clusters and labeling 
kmeans_fit = kmeans(cluster_frame, 4)
kmeans_fit$centers
kmeans_fit$size
```



```{r}
#### principal componant for visual 
principal_comp <- prcomp(cluster_frame,scale.=TRUE)
round(principal_comp$rotation[,1:2],2) 
cluster_plot <- cluster_frame
cluster_plot$cluster <- as.factor(kmeans_fit$cluster)

#### plotting visual 
autoplot(principal_comp, data = cluster_plot, colour = 'cluster')

ggplot(cluster_plot, aes(x=sys_mean, y=dia_mean, color=cluster)) + geom_point()
```

```{r}

sys_tree <- subset(lin_frame_clean, select=-c(dia_mean,ridreth3,mult))
sys_tree$cluster <- kmeans_fit$cluster

for(i in 1:5)
{
  cluster_data <- sys_tree %>% filter(cluster == i)
  #### using information criteria:
  trctrl = trainControl(method = "repeatedcv", number = 10, repeats = 3)
  tree_information = train(sys_mean~., 
                                data = cluster_data, method = "rpart", 
                                parms = list(split = "information"), 
                                trControl=trctrl, tuneLength = 10)
  
  assign(paste0("cluster_tree",i),tree_information)  
}



#### Fairly similar performance, but information looks slightly better. 

#### Looking at variable performance: 
  #### cluster 1 
  prp(cluster_tree1$finalModel, 
      box.palette = "Blues", 
      tweak = 1.2)
  
  #### cluster 2 
  prp(cluster_tree2$finalModel, 
    box.palette = "Blues", 
    tweak = 1.2)
  
  #### cluster 3 
  prp(cluster_tree3$finalModel, 
    box.palette = "Blues", 
    tweak = 1.2)
  
  #### cluster 4 
  prp(cluster_tree4$finalModel, 
    box.palette = "Blues", 
    tweak = 1.2)

  
#####CLUSTERING ON DEMOGRAPHIC 
#####PREDICTION ON BIO 
#####hypertension variable 
##### assumption 
```

